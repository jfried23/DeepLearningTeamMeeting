{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Cats vs Dogs</h3>\n",
    "<p> \n",
    "We will be doing the <a href='https://www.kaggle.com/c/dogs-vs-cats'>Cats vs Dogs kaggle data challange</a>. With just a few lines of code and about 20 minutes of work, we can use Transfer Learning and the concept of Embedding to build a model that would have placed us in the top 20 of all challange entries circa 2013.</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>VGG16</h3>\n",
    "<p><a href='https://arxiv.org/abs/1409.1556'>VGG16</a> is the name of a Neural Network Architecture for image recognition that won the <a href='http://www.image-net.org/challenges/LSVRC/'>ImageNet  ILSVRC-2014 competition</a>. It is no longer the best preforming architecture for image recognition but it is among the simplest to tinker with and consequently it is widely used.</p>\n",
    "<p>Training an architecture like VGG16 from scratch is no trivial task, but fortunately both the weights and architecture of VGG16 are open source! Within a couple lines of code/a few minutes we can load up the VGG16 model that won imagenet 2014 and use it to do anything we want! This is the basis of transfer learning. We will use the feature extraction strategy learned by VGG16 on one set of images, and transfer it generate features for a completely different image set saving months of work and likley many thousands of dollars!</p>\n",
    "<p>Because it is so commonly used keras makes it easy to import the vgg16 along with its weights. See  documentation for <b>keras.applications.VGG16</b> for details.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.applications.VGG16??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_25 (InputLayer)        (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "x=keras.layers.Input((224,224,3))\n",
    "vgg16 = keras.applications.VGG16( \n",
    "                                 input_tensor=x,\n",
    "                                 )\n",
    "vgg16.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Unfortunately the images VGG was trained on had a different mean and color dimension ordering than the dog & cat images we are working with now. We could preprocess all the data before loading it into the model or we could do it on the fly while learning how to implenet a simple custom keras layer(called a Lambda layer). Lets build a lambda layer that will normalize the color channels and do necessary transformation as part of our model.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_mean = np.array([123.68, 116.779, 103.939], dtype=np.float32).reshape((1,1,3))\n",
    "vgg_norm = keras.layers.Lambda(lambda x: x[:,:,::-1]-vgg_mean, \n",
    "                               output_shape=lambda x: x,\n",
    "                              name='VGG_Norm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The imagenet competition our VGG model was optimized for sought to classify images into one of a 1,000 different catagories. Consiquently out VGG16 model generates a (1x1000) output vector for each input. (See model summary above) Lets modify our model to keep only the stages to do feature extraction.</p>\n",
    "<p>Recent advances have made the last five layers in the vgg network more or less obsolete, so we can lop those off as well, and replace them with something more modern! While we are at it we can incorperate the custom layer above to prepare our image data.</p>\n",
    "\n",
    "<p>In the code block below we will\n",
    "    <ol><li>Apply custum vgg_norm layer to the model input.</li>\n",
    "        <li>Pass our normzlied inputs through the convolutional filters devloped by the VGG creators</li>\n",
    "        <li>Pass those convolutional filters through a keras.layers.GlobalAveragePooling2D object.</li>\n",
    "    </ol>\n",
    "   </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pass input through our custom normalization layer\n",
    "input_image=keras.layers.Input((224,224,3))\n",
    "x=vgg_norm(input_image)\n",
    "\n",
    "#Throw away the last 5 layers of vgg and pass x through the remaining layers\n",
    "#Set each vgg layer to trainable=False so keras will not change those weights!\n",
    "keep_layers = [l for l in vgg16.layers[0:len(vgg16.layers)-5]]\n",
    "for l in keep_layers:\n",
    "    l.trainable=False\n",
    "    x = l(x)\n",
    "    \n",
    "#Now add on the new layer\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_28 (InputLayer)        (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "VGG_Norm (Lambda)            (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "input_25 (InputLayer)        (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_18  (None, 512)               0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Create our new vgg model!\n",
    "my_vgg = keras.models.Model(inputs=input_image, outputs=x)\n",
    "my_vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=np.load('/Users/frjo6001/Documents/DeepLearningTeamMeeting/dogs_v_cats/dset.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Running 5k images through vgg16 took ~36 minutes on my laptop cpu. You can run it if you like on both the test & training set data but I would rather not.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 2200s 440ms/step\n"
     ]
    }
   ],
   "source": [
    "out = my_vgg.predict(d['x_test'],verbose=1,batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Fortunatly we gpu on odin, and running the same computation there took me ~30 seconds! You can download the (1x512) outputs for each image in the numpy archive <a href='https://drive.google.com/drive/folders/1fe5uNIqvJXO8HZZ6E8Ai5JEf4hoWRCcE?usp=sharing'>image_embedings.npz located here</a>. So lets just load these in and work with them.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('/Users/frjo6001/Documents/DeepLearningTeamMeeting/dogs_v_cats/image_embedings.npz')\n",
    "\n",
    "x_train = data['x_train']\n",
    "y_train = data['y_train']\n",
    "\n",
    "x_test = data['x_test']\n",
    "y_test = data['y_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "To build a classifyer on-top of these predictions we need to only apply:\n",
    "<ol>\n",
    "    <li><b>keras.layers.Input</b>   -- to stub out our input data</li>\n",
    "    <li><b>keras.layers.Dropout</b> -- to prevent overfitting.</li>\n",
    "    <li><b>keras.layers.Dense</b>   -- to transform our features into a binary 0/1 (cat/dog) flag!</li>\n",
    "</ol>\n",
    "Honestly a (pick you favorate package in sklearn) would probably work just as well here, but lets get the practice building models in keras!\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 1s 60us/step - loss: 0.5557 - acc: 0.8877 - val_loss: 0.0872 - val_acc: 0.9722\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 1s 40us/step - loss: 0.1993 - acc: 0.9447 - val_loss: 0.0840 - val_acc: 0.9720\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 1s 41us/step - loss: 0.1593 - acc: 0.9509 - val_loss: 0.0695 - val_acc: 0.9748\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 1s 41us/step - loss: 0.1395 - acc: 0.9523 - val_loss: 0.0598 - val_acc: 0.9776\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 1s 41us/step - loss: 0.1367 - acc: 0.9506 - val_loss: 0.0712 - val_acc: 0.9736\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12d15fc50>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = keras.layers.Input((512,))\n",
    "x = keras.layers.Dropout(0.5)(inp)\n",
    "out = keras.layers.Dense(1,activation=keras.activations.sigmoid)(x)\n",
    "model = keras.models.Model(inp,out)\n",
    "model.compile(keras.optimizers.Adam(), keras.losses.binary_crossentropy,metrics=['accuracy'])\n",
    "model.fit( x=x_train, y=y_train,\n",
    "           batch_size=32, epochs=5, \n",
    "           validation_data=(x_test,y_test),\n",
    "           verbose=1\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
